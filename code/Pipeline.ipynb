{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a34777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import csv\n",
    "import io\n",
    "import argparse\n",
    "\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/rashika/CAFA4/InformationAccretion/\")\n",
    "sys.path.append(\"/home/rashika/CAFA4/CAFA-evaluator/src/cafaeval/\")\n",
    "from parser import *\n",
    "from ia import *\n",
    "from make_benchmarks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5685e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/          graph.py     __main__.py  \u001b[01;34m__pycache__\u001b[0m/\r\n",
      "evaluation.py  __init__.py  parser.py\r\n"
     ]
    }
   ],
   "source": [
    "ls /home/rashika/CAFA4/CAFA-evaluator/src/cafaeval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc289316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "#preprocess_cmd = \"python3 run_preprocess_parallel.py\"\n",
    "#result = subprocess.run(preprocess_cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff101f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# Modify preprocess_cmd to take all the input/output/log paths as inputs\n",
    "# Incorporate in the code of main() block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e696c0",
   "metadata": {},
   "source": [
    "# To do \n",
    "\n",
    "Do the system commands more elegantly, parallelise the evaluation, incorporate the bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fc4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Read a the processed annotation file, map the primary ID to a mapping file, and keep the rows that can be mapped.\n",
    "\n",
    "    Parameters:\n",
    "    - processed_file_path: Path to the file containing processed GOA annotations\n",
    "    - mapping_file: Path to the mapping file \n",
    "    - out_path: Path to the output file.\n",
    "    \"\"\"\n",
    "def goa_to_CAFA4ID(processed_file_path, mapping_file, out_path):\n",
    "    ann = pd.read_csv(processed_file_path, sep = '\\t', header = 0)\n",
    "    mapping = pd.read_csv(mapping_file, sep = '\\t', header = 0)\n",
    "\n",
    "    # Inner join the processed annotations and the mapping based on DB Object ID\n",
    "    mapped =  pd.merge(ann, mapping, on='DB Object ID', how='inner')\n",
    "    \n",
    "    \n",
    "    # Keep the required columns\n",
    "    mapped = mapped[[\"CAFA4_ID\", \"GO ID\", \"Aspect\"]]\n",
    "    \n",
    "    # Write the mapped file to the out_path\n",
    "    mapped.to_csv(out_path, sep = \"\\t\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocess_cmd(gaf_path, out_path):\n",
    "    cmd = [\n",
    "    \"python3\",                 # Command to execute Python 3\n",
    "    \"preprocess_gaf.py\",       # Script to run\n",
    "    gaf_path,  # Path to input file\n",
    "    \"--highTP\",\n",
    "    \"--out_path\", out_path,        # Output path parameter\n",
    "    #\"--evidence_codes\", \"EXP\", \"IDA\",   # Evidence codes parameter\n",
    "    #\"--extract_col_list\", \"DB Object ID\", \"Qualifier\"  # Extract column list parameter\n",
    "]\n",
    "    return cmd\n",
    "\n",
    "def run_process(command, log_file):\n",
    "    with open(log_file, \"w\") as f:\n",
    "        print(command)\n",
    "        result = subprocess.run(\" \".join(command), shell=True, stdout=f, stderr=subprocess.STDOUT)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define commands and log file names\n",
    "    work_dir = \"/data/rashika/CAFA4/\"\n",
    "    \n",
    "    t0_gaf_file = work_dir + \"uniprot/raw_goa/t0/goa_uniprot_all.gaf.195.gz\" # The latest Uniprot file before t1 ( 2019-12-17)\n",
    "    t0_processed = work_dir + \"extracted_goa/t0_preprocessed.csv\"\n",
    "    log_t0 =  work_dir + \"log/log_preprocess_t0.txt\"\n",
    "    \n",
    "    #t1_gaf_file = work_dir + \"uniprot/raw_goa/t1/goa_uniprot_all.gaf.gz\" # The file from UniProt (2024-02-09)\n",
    "    t1_gaf_file = work_dir + \"uniprot/uniprot_2024_2024-04-16/goa_uniprot_all.gaf.gz\" # The file from UniProt 2024-04-16\n",
    "    t1_processed = work_dir + \"extracted_goa/t1_preprocessed.csv\"\n",
    "    log_t1 = work_dir + \"log/log_preprocess_t1.txt\"\n",
    "    \n",
    "    \n",
    "    cmd_preprocess_t0 = get_preprocess_cmd(t0_gaf_file, t0_processed)\n",
    "    cmd_preprocess_t1 = get_preprocess_cmd(t1_gaf_file, t1_processed)\n",
    "    \n",
    "    # Create processes for each command\n",
    "    process1 = multiprocessing.Process(target=run_process, args=(cmd_preprocess_t0, log_t0))\n",
    "    process2 = multiprocessing.Process(target=run_process, args=(cmd_preprocess_t1, log_t1))\n",
    "    \n",
    "    # Start the processes\n",
    "    #process1.start()\n",
    "    #process2.start()\n",
    "\n",
    "    # Wait for both processes to finish\n",
    "    #process1.join()\n",
    "    #process2.join()\n",
    "\n",
    "    #print(\"Both processes have finished.\")\n",
    "    \n",
    "    # Map the IDs of the processed \n",
    "    \n",
    "    work_dir = \"/data/rashika/CAFA4/\"\n",
    "    mapping_file = \"/data/rashika/CAFA4/CAFA4-export/AC2CAFA4ID.map\"\n",
    "    t0_mapped_path = work_dir + \"mapped/t0.csv\"\n",
    "    t1_mapped_path = work_dir + \"/mapped/t1.csv\"\n",
    "    \n",
    "    # Map to CAFA4 IDs \n",
    "    #goa_to_CAFA4ID(t0_processed , mapping_file, t0_mapped_path)\n",
    "    #goa_to_CAFA4ID(t1_processed , mapping_file, t1_mapped_path)\n",
    "    \n",
    "    # Create the benchmarks\n",
    "    roots = {'BPO': 'GO:0008150', 'CCO': 'GO:0005575', 'MFO': 'GO:0003674'}\n",
    "\n",
    "    #eval_path = work_dir + \"eval/\"\n",
    "\n",
    "    t0_ont_file = '/data/rashika/CAFA4/uniprot/go_2019_12_09/go-basic.obo' # data-version: releases/2020-01-01\n",
    "    t0_ont_graph = clean_ontology_edges(obonet.read_obo(t0_ont_file))     \n",
    "    \n",
    "    t1_ont_file = \"/data/rashika/CAFA4/uniprot/go_2024_03_28/go-basic.obo\"\n",
    "    t1_ont_graph = clean_ontology_edges(obonet.read_obo(t1_ont_file)) # data-version: releases/2024-01-17\n",
    "    \n",
    "    t_minus_ont_file =  \"/data/rashika/CAFA4/uniprot/go_2019_10_07/go-basic.obo\"\n",
    "    t_minus_1_ont_graph = clean_ontology_edges(obonet.read_obo(t_minus_ont_file))\n",
    "\n",
    "    # Create BM lists\n",
    "    eval_path = '/data/rashika/CAFA4/eval_final/'\n",
    "    BM_GO_path = eval_path + \"BM_GO/\"\n",
    "    common_path = '/data/rashika/CAFA4/common/'\n",
    "    create_bm_lists(t0_mapped_path, t1_mapped_path, t0_ont_graph, t1_ont_graph, t_minus_1_ont_graph, roots, BM_GO_path, common_path, remove_protein_binding = True)\n",
    "\n",
    "\n",
    "    # Calculate IA\n",
    "    IA_file =  eval_path + \"IA.txt\"\n",
    "    #print(IA_file)\n",
    "    #cmd = 'python3 /home/rashika/CAFA4/InformationAccretion/ia.py --annot ' + t0_processed + ' --graph '+ t_minus_ont_file + ' --outfile ' + IA_file + ' --prop' \n",
    "    #os.system(cmd)\n",
    "    \n",
    "    pred_dir = \"/data/yisupeng/sharing/cafa4/all_models/\"\n",
    "    \n",
    "    result_path = eval_path + \"eval_results/\"\n",
    "    #run_eval(BM_GO_path, pred_dir, t_minus_ont_file, IA_file, result_path, log_path = '/home/rashika/CAFA4/eval/log/', thresh_step = 0.001)\n",
    "    #cmd = 'python3 /home/rashika/CAFA4/CAFA-evaluator/src/cafaeval/__main__.py /data/rashika/CAFA4/uniprot/go_2019_10_07/go-basic.obo /data/yisupeng/sharing/cafa4/all_models/ /data/rashika/CAFA4/eval_final/BM_GO/bpo_all_type3.txt -out_dir /data/rashika/CAFA4/eval_final/eval_results/bpo_all_type3/ -ia /data/rashika/CAFA4/eval_final/IA.txt -prop max -th_step 0.01 -no_orphans > /home/rashika/CAFA4/eval/log/bpo_all_type3/run.log &'\n",
    "    #os.system(cmd)\n",
    "    # Paths\n",
    "#     plots_path =  eval_path + 'plots_ALL/'\n",
    "#     if not os.path.exists(plots_path):\n",
    "#         os.mkdir(plots_path)\n",
    "#     plots_path_f_w = plots_path+'f_w/'\n",
    "#     plots_path_f = plots_path+'f/'\n",
    "#     plots_path_f_micro = plots_path+'f_micro/'\n",
    "#     plots_path_f_micro_w = plots_path+'f_micro_w/'\n",
    "#     plots_path_s_w = plots_path+'s_w/'\n",
    "#     register = '/data/rashika/CAFA4/file_map.tsv'\n",
    "\n",
    "\n",
    "#     S_min_coord = {}\n",
    "#     S_min_coord['bpo_all_type1'] = [[18, 30], [0, 50]]\n",
    "#     S_min_coord['bpo_all_type2'] = [[18, 27], [0, 200]]\n",
    "#     S_min_coord['bpo_all_type3'] = [[12, 22.5], [0, 200]]\n",
    "#     S_min_coord['bpo_all_type12'] = [[18, 28], [0, 100]]\n",
    "#     S_min_coord['cco_all_type1'] = [[7, 12], [0, 50]]\n",
    "#     S_min_coord['cco_all_type2'] = [[6, 11], [0, 50]]\n",
    "#     S_min_coord['cco_all_type3'] = [[6, 12], [0, 50]]\n",
    "#     S_min_coord['cco_all_type12'] = [[6.5, 12], [0, 50]]\n",
    "#     S_min_coord['mfo_all_type1'] = [[3, 6], [0, 50]]\n",
    "#     S_min_coord['mfo_all_type2'] = [[4, 7.5], [0, 50]]\n",
    "#     S_min_coord['mfo_all_type3'] = [[7, 11], [0, 50]]\n",
    "#     S_min_coord['mfo_all_type12'] = [[3.5, 7], [0, 50]]\n",
    "\n",
    "    #metric, cols = ('f_w', ['rc_w', 'pr_w'])\n",
    "    #create_plots(result_path, metric, cols, out_path= plots_path_f_w, n_curves = 10, names_file = register)\n",
    "\n",
    "    #metric, cols = ('f', ['rc', 'pr'])\n",
    "    #create_plots(result_path, metric, cols, out_path = plots_path_f, n_curves = 10, names_file =register)\n",
    "\n",
    "    #metric, cols =  ('f_micro_w', ['rc_micro_w', 'pr_micro_w'])\n",
    "    #create_plots(result_path, metric, cols, out_path = plots_path_f_micro_w, n_curves = 10, names_file =register)\n",
    "\n",
    "#     metric, cols =  ('f_micro', ['rc_micro', 'pr_micro'])\n",
    "#     create_plots(result_path, metric, cols, out_path = plots_path_f_micro, n_curves = 10, names_file =register)\n",
    "\n",
    "    #metric, cols = ('s_w', ['ru_w', 'mi_w'])\n",
    "    #create_plots(result_path, metric, cols, out_path = plots_path_s_w, n_curves = 10, names_file =register, S_min_coord = S_min_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f7540",
   "metadata": {},
   "source": [
    "## Test the figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bde25",
   "metadata": {},
   "outputs": [],
   "source": [
    "[dir_list = os.listdir(results_path)\n",
    "    \n",
    "    [cumulate = True\n",
    "    add_extreme_points = True\n",
    "    coverage_threshold = 0.3\n",
    "    axis_title_dict = {'pr': 'Precision', 'rc': 'Recall', 'f': 'F-score', 'pr_w': 'Weighted Precision', 'rc_w': 'Weighted Recall', 'f_w': 'Weighted F-score', 'mi': 'Misinformation (Unweighted)', 'ru': 'Remaining Uncertainty (Unweighted)', 'mi_w': 'Misinformation', 'ru_w': 'Remaining Uncertainty', 's': 'S-score', 'pr_micro': 'Precision (Micro)', 'rc_micro': 'Recall (Micro)', 'f_micro': 'F-score (Micro)', 'pr_micro_w': 'Weighted Precision (Micro)', 'rc_micro_w': 'Weighted Recall (Micro)', 'f_micro_w': 'Weighted F-score (Micro)'}\n",
    "    ontology_dict = {'biological_process': 'BPO', 'molecular_function': 'MFO', 'cellular_component': 'CCO'}\n",
    "    \n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "    \n",
    "    dir_list.remove('bpo_all_type3')\n",
    "    for file in dir_list:\n",
    "        df_file = results_path + file +\"/evaluation_all.tsv\"\n",
    "        df = pd.read_csv(df_file, sep=\"\\t\")\n",
    "        out_folder = out_path + file\n",
    "        if not os.path.exists(out_folder):\n",
    "            os.mkdir(out_folder)\n",
    "            \n",
    "        \n",
    "        df = pd.read_csv(df_file, sep=\"\\t\")\n",
    "        \n",
    "        # Set method information (optional)\n",
    "        if names_file is None:\n",
    "            df['group'] = df['filename']\n",
    "            df['label'] = df['filename']\n",
    "            df['is_baseline'] = False\n",
    "        else:\n",
    "            methods = pd.read_csv(names_file, sep = \"\\t\", header=0)\n",
    "            df = pd.merge(df, methods, on='filename', how='left')\n",
    "            df['group'].fillna(df['filename'], inplace=True)\n",
    "            df['label'].fillna(df['filename'], inplace=True)\n",
    "            if 'is_baseline' not in df:\n",
    "                df['is_baseline'] = False\n",
    "            else:\n",
    "                df['is_baseline'].fillna(False, inplace=True)\n",
    "            # print(methods)\n",
    "        #df = df.drop(columns='filename').set_index(['group', 'label', 'ns', 'tau'])\n",
    "        df = df.set_index(['group_unique', 'label', 'ns', 'filename','tau'])\n",
    "        \n",
    "        # Filter by coverage\n",
    "        df = df[df['cov'] >= coverage_threshold]\n",
    "        \n",
    "        # Assign colors based on group\n",
    "        cmap = plt.get_cmap('tab20')\n",
    "        df['colors'] = df.index.get_level_values('group_unique')\n",
    "        df['colors'] = pd.factorize(df['colors'])[0]\n",
    "        df['colors'] = df['colors'].apply(lambda x: cmap.colors[x % len(cmap.colors)])\n",
    "        \n",
    "        index_best = df.groupby(level=['group_unique', 'ns'])[metric].idxmax() if metric in ['f', 'f_w', 'f_micro', 'f_micro_w'] else df.groupby(['group_unique', 'ns'])[metric].idxmin()\n",
    "        \n",
    "        # Filter the dataframe for the best methods\n",
    "        df_methods = df.reset_index('tau').loc[[ele[:-1] for ele in index_best], ['tau', 'cov', 'colors'] + cols + [metric]].sort_index()\n",
    "\n",
    "        # Makes the curves monotonic. Cumulative max on the last column of the cols variable, e.g. \"pr\" --> precision\n",
    "        if cumulate:\n",
    "            if metric in ['f', 'f_w', 'f_micro', 'f_micro_w']:\n",
    "                df_methods[cols[-1]] = df_methods.groupby(level=['label', 'ns'])[cols[-1]].cummax()\n",
    "            else:\n",
    "                df_methods[cols[-1]] = df_methods.groupby(level=['label', 'ns'])[cols[-1]].cummin()\n",
    "\n",
    "\n",
    "        # Save to file\n",
    "        df_methods.drop(columns=['colors']).to_csv('{}/fig_{}.tsv'.format(out_folder, metric), float_format=\"%.3f\", sep=\"\\t\")\n",
    "        \n",
    "        # Add first last points to precision and recall curves to improve APS calculation\n",
    "        def add_points(df_):\n",
    "            df_ = pd.concat([df_.iloc[0:1], df_])\n",
    "            df_.iloc[0, df_.columns.get_indexer(['tau', cols[0], cols[1]])] = [0, 1, 0]  # tau, rc, pr\n",
    "            df_ = pd.concat([df_, df_.iloc[-1:]])\n",
    "            df_.iloc[-1, df_.columns.get_indexer(['tau', cols[0], cols[1]])] = [1.1, 0, 1]\n",
    "            return df_\n",
    "\n",
    "        if metric.startswith('f') and add_extreme_points:\n",
    "            df_methods = df_methods.reset_index().groupby(['group_unique', 'label', 'ns'], as_index=False).apply(add_points).set_index(['group_unique', 'label', 'ns'])\n",
    "        \n",
    "        # Filter the dataframe for the best method and threshold\n",
    "        df_best = df.loc[index_best, ['cov', 'colors'] + cols + [metric]]\n",
    "        \n",
    "        # Calculate average precision score \n",
    "        if metric.startswith('f'):\n",
    "            df_best['aps'] = df_methods.groupby(level=['group_unique', 'label', 'ns'])[[cols[0], cols[1]]].apply(lambda x: (x[cols[0]].diff(-1).shift(1) * x[cols[1]]).sum())\n",
    "\n",
    "        # Calculate the max coverage across all thresholds\n",
    "        df_best['max_cov'] = df_methods.groupby(level=['group_unique', 'label', 'ns'])['cov'].max()\n",
    "        \n",
    "        # Set a label column for the plot legend\n",
    "        df_best['label'] = df_best.index.get_level_values('label')\n",
    "        if 'aps' not in df_best.columns:\n",
    "            df_best['label'] = df_best.agg(lambda x: f\"{x['label']} ({metric.upper()}={x[metric]:.3f} C={x['max_cov']:.3f})\", axis=1)\n",
    "        else:\n",
    "            df_best['label'] = df_best.agg(lambda x: f\"{x['label']} ({metric.upper()}={x[metric]:.3f} APS={x['aps']:.3f} C={x['max_cov']:.3f})\", axis=1)\n",
    "        \n",
    "        # Generate the figures\n",
    "        plt.rcParams.update({'font.size': 22, 'legend.fontsize': 18})\n",
    "\n",
    "        # F-score contour lines\n",
    "        x = np.arange(0.01, 1, 0.01)\n",
    "        y = np.arange(0.01, 1, 0.01)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = 2 * X * Y / (X + Y)\n",
    "\n",
    "        \n",
    "        for ns, df_g in df_best.groupby(level='ns'):\n",
    "            fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "             # Contour lines. At the moment they are provided only for the F-score\n",
    "            if metric.startswith('f'):\n",
    "                CS = ax.contour(X, Y, Z, np.arange(0.1, 1.0, 0.1), colors='gray')\n",
    "                ax.clabel(CS, inline=True) #, fontsize=10)\n",
    "\n",
    "            cnt = 0\n",
    "            # Iterate methods\n",
    "            for i, (index, row) in enumerate(df_g.sort_values(by=[metric, 'max_cov'], ascending=[False if metric.startswith('f') else True, False]).iterrows()):\n",
    "                \n",
    "                #data = df_methods.loc[index[:-1]]\n",
    "                data = df_methods.loc[index[:-2]]\n",
    "                print(row[cols[0]], row[cols[1]])\n",
    "\n",
    "                # Precision-recall or mi-ru curves\n",
    "                ax.plot(data[cols[0]], data[cols[1]], color=row['colors'], label=row['label'], lw=2, zorder=500-i)\n",
    "\n",
    "                # F-max or S-min dots\n",
    "                ax.plot(row[cols[0]], row[cols[1]], color=row['colors'], marker='o', markersize=12, mfc='none', zorder=1000-i)\n",
    "                ax.plot(row[cols[0]], row[cols[1]], color=row['colors'], marker='o', markersize=6, zorder=1000-i)\n",
    "\n",
    "                cnt+=1\n",
    "                if n_curves and cnt >= n_curves:\n",
    "                    break\n",
    "                \n",
    "            # Set axes limit\n",
    "            if metric.startswith('f'):\n",
    "                plt.xlim(0, 1)\n",
    "                plt.ylim(0, 1)\n",
    "\n",
    "            # plt.xlim(0, max(1, df_best.loc[:,:,ns,:][cols[0]].max()))\n",
    "            # plt.ylim(0, max(1, df_best.loc[:,:,ns,:][cols[1]].max()))\n",
    "\n",
    "            # Set titles\n",
    "            ax.set_title(file)\n",
    "            ax.set_xlabel(axis_title_dict[cols[0]], labelpad=20)\n",
    "            ax.set_ylabel(axis_title_dict[cols[1]], labelpad=20)\n",
    "\n",
    "            # Legend\n",
    "            # ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            leg = ax.legend(markerscale=6, title=file)\n",
    "            for legobj in leg.get_lines():\n",
    "                legobj.set_linewidth(10.0)\n",
    "                \n",
    "            leg.set_bbox_to_anchor((1.05, 1))  \n",
    "\n",
    "            # Save figure on disk\n",
    "            plt.savefig(\"{}/fig_{}_{}.png\".format(out_folder, metric, ns), bbox_inches='tight', dpi=300, transparent=True)\n",
    "            # plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b943a",
   "metadata": {},
   "source": [
    "## Test Clara's function against Damiano's parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "ont_file = '/data/rashika/CAFA4/uniprot/go_2024_03_28/go-basic.obo' \n",
    "data = '/data/rashika/CAFA4/extracted_goa/t1_preprocessed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68235054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clara's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ont_graph = clean_ontology_edges(obonet.read_obo(ont_file))\n",
    "subontologies = {aspect: fetch_aspect(ont_graph, roots[aspect]) for aspect in roots}\n",
    "\n",
    "ann = pd.read_csv(data, sep=\"\\t\")\n",
    "ann.columns = ['EntryID', 'term', 'aspect']\n",
    "aspect_mapping = {\n",
    "    'C': 'CCO',\n",
    "    'F': 'MFO',\n",
    "    'P': 'BPO'}\n",
    "    \n",
    "ann['aspect'] = ann['aspect'].map(aspect_mapping)\n",
    "ann_prop = propagate_terms(ann, subontologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d03fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ann_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ad56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damiano's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia = None\n",
    "no_orphans = False\n",
    "# Parse the OBO file and creates a different graphs for each namespace\n",
    "ontologies = obo_parser(ont_file, (\"is_a\", \"part_of\"), ia, not no_orphans)\n",
    "\n",
    "# Parse ground truth file\n",
    "#gt = gt_parser(data, ontologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7526ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gt['biological_process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81addbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(gt['biological_process'].matrix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a733664",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(gt['biological_process'].matrix, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b937ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontologies[ns].terms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7dd991",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontologies = obo_parser(ont_file, valid_rel=(\"is_a\", \"part_of\"), ia_file=None, orphans=True)\n",
    "ground_truth = gt_parser(data, ontologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gt_as_df(gt, ontologies):\n",
    "        dfs = []\n",
    "        term_info = []\n",
    "        for ns in ontologies:\n",
    "            for index, p_id in enumerate(gt[ns].ids):\n",
    "                GO_terms = list(ontologies[ns].terms_dict.keys())\n",
    "                GO_terms = pd.DataFrame(GO_terms, columns = ['term'])\n",
    "                GO_terms_In_p = gt[ns].matrix[index]==True\n",
    "                GO_terms = GO_terms.loc[GO_terms_In_p]\n",
    "                GO_terms['aspect'] = ns\n",
    "                GO_terms['EntryID'] = p_id\n",
    "                dfs.append(GO_terms)\n",
    "        dfs = pd.concat(dfs)\n",
    "        dfs = dfs.loc[:, [\"EntryID\", \"term\", \"aspect\"]].copy()\n",
    "        return dfs\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "#ground_truth = ground_truth['biological_process']\n",
    "\n",
    "\n",
    "dfs = gt_as_df(ground_truth,ontologies)\n",
    "    #for protein_id, term_id in term_info:\n",
    "        #print(\"Protein ID: {}, Term ID: {}\".format(protein_id, term_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fe764",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_mapping = {\n",
    "    'cellular_component' :'CCO',\n",
    "    'molecular_function': 'MFO',\n",
    "    'biological_process': 'BPO'}\n",
    "    \n",
    "dfs['aspect'] = dfs['aspect'].map(aspect_mapping)\n",
    "display(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find elements in df1 that are not in df2\n",
    "df1_unique = pd.concat([dfs, ann_prop]).drop_duplicates(keep=False)\n",
    "df1_unique = df1_unique.dropna()  # Drop rows with NaN values, if any\n",
    "\n",
    "# Find elements in df2 that are not in df1\n",
    "df2_unique = pd.concat([ann_prop, dfs]).drop_duplicates(keep=False)\n",
    "df2_unique = df2_unique.dropna()  # Drop rows with NaN values, if any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_info = get_term_info(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_info \n",
    "i = 0\n",
    "terms = []\n",
    "for term_id, score, aspect in gt_info:\n",
    "    print(\"Term ID: {}, Score: {}, Aspect: {}\".format(term_id, score, aspect))\n",
    "    i+=1\n",
    "    terms.append(term_id)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57eaa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt['biological_process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c06783",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../../CAFA-evaluator/src/cafaeval/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3edaa",
   "metadata": {},
   "source": [
    "Bootstrapping Development on sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d76d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ont_file = '/data/rashika/CAFA4/uniprot/goa_2020_Jan_03/go-basic.obo' # data-version: releases/2020-01-01\n",
    "#pred_dir = '/data/rashika/CAFA4/pred_sample/' # 20 methods\n",
    "pred_dir ='/home/rashika/CAFA4/one/' # 1 method\n",
    "BM_GO_file = '/data/rashika/CAFA4/eval/BM_GO/cco_all_type1.txt'\n",
    "IA_file =  \"/data/rashika/CAFA4/eval/IA/IA.txt\"\n",
    "out_dir = \"/data/rashika/CAFA4/test_b\"\n",
    "\n",
    "\n",
    "cmd = \"python3 /home/rashika/CAFA4/CAFA-evaluator/src/cafaeval/__main__.py \"+ ont_file +\" \"+ pred_dir + \" \" + BM_GO_file + \" -out_dir \" + out_dir + ' -ia ' + IA_file + \" -prop max -th_step 0.01  -no_orphans -b 10\" + \" &\"\n",
    "print(cmd)\n",
    "#os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/rashi/Documents/Academics/Research/CAFA4_eval/Damiano_code/test_data/go-basic.obo /Users/rashi/Documents/Academics/Research/CAFA4_eval/Damiano_code/test_data/one/ /Users/rashi/Documents/Academics/Research/CAFA4_eval/Damiano_code/test_data/cco_all_type1.txt -out_dir /Users/rashi/Documents/Academics/Research/CAFA4_eval/Damiano_code/output -ia /Users/rashi/Documents/Academics/Research/CAFA4_eval/Damiano_code/test_data/IA.txt -prop max -th_step 0.01  -no_orphans -b 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99253401",
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = {'BPO': 'GO:0008150', 'CCO': 'GO:0005575', 'MFO': 'GO:0003674'}\n",
    "\n",
    "work_dir = \"/data/rashika/CAFA4/\"\n",
    "t0_mapped_path = work_dir + \"mapped/t0.csv\"\n",
    "t1_mapped_path = work_dir + \"/mapped/t1.csv\"\n",
    "\n",
    "t0_ont_file = '/data/rashika/CAFA4/uniprot/goa_2020_Jan_03/go-basic.obo' # data-version: releases/2020-01-01\n",
    "t0_ont_graph = clean_ontology_edges(obonet.read_obo(t0_ont_file)) \n",
    "    \n",
    "t1_mapped_ann =  \"/data/rashika/CAFA4/mapped/t1_mapped.csv\"\n",
    "t1_ont_graph = clean_ontology_edges(obonet.read_obo( \"/data/rashika/CAFA4/uniprot/goa_2024-02-09/go-basic.obo\")) # data-version: releases/2024-01-17\n",
    "    \n",
    "#Prop t0 and t1 in their respective ontologies\n",
    "t0_prop = process_raw_annot(t0_mapped_path, t0_ont_graph, roots, remove_roots = False)\n",
    "t1_prop = process_raw_annot(t1_mapped_path, t1_ont_graph, roots, remove_roots = False)\n",
    "    \n",
    "# Keep common terms\n",
    "t0_common, t1_common =  keep_common_go_terms(t0_prop, t1_prop, t0_ont_graph, t1_ont_graph)\n",
    "t0_common.to_csv('/data/rashika/CAFA4/common/t0.tsv', sep = '\\t', header = False, index = False)\n",
    "t1_common.to_csv('/data/rashika/CAFA4/common/t1.tsv', sep = '\\t', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a0b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the plots\n",
    "# Results of 5 methods\n",
    "data_path = result_path\n",
    "plots_path = \"/data/rashika/CAFA4/eval/\" + \"plots/\"\n",
    "plots_path_f_w = plots_path+'f_w/'\n",
    "plots_path_f = plots_path+'f/'\n",
    "plots_path_f_micro_w = plots_path+'f_micro_w/'\n",
    "plots_path_s_w = plots_path+'s_w/'\n",
    "register = '/data/rashika/CAFA4/file_map.tsv'\n",
    "\n",
    "metric, cols = ('f_w', ['rc_w', 'pr_w'])\n",
    "#create_plots(results_path, metric, cols,out_path='/home/rashika/CAFA4/eval/plots/', n_curves = None, names_file = None):\n",
    "create_plots(data_path, metric, cols, out_path = plots_path_f_w, n_curves = 10, names_file = register)\n",
    "\n",
    "metric, cols = ('f', ['rc', 'pr'])\n",
    "create_plots(data_path, metric, cols, out_path = plots_path_f, n_curves = 10, names_file = register)\n",
    "\n",
    "metric, cols =  ('f_micro_w', ['rc_micro_w', 'pr_micro_w'])\n",
    "create_plots(data_path, metric, cols, out_path = plots_path_f_micro_w, n_curves = 10, names_file = register)\n",
    "\n",
    "metric, cols = ('s_w', ['ru_w', 'mi_w'])\n",
    "create_plots(data_path, metric, cols, out_path = plots_path_s_w, n_curves = 10, names_file = register)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94208e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = {'BPO': 'GO:0008150', 'CCO': 'GO:0005575', 'MFO': 'GO:0003674'}\n",
    "\n",
    "t0_ont_file = '/data/rashika/CAFA4/uniprot/goa_2020_Jan_03/go-basic.obo' # data-version: releases/2020-01-01\n",
    "t0_ont_graph = clean_ontology_edges(obonet.read_obo(t0_ont_file)) \n",
    "shawn_t0_mapped_ann = \"/data/rashika/CAFA4/CAFA4_gt/t0_mapped.csv\"\n",
    "\n",
    "t1_mapped_ann = \"/data/rashika/CAFA4/CAFA4_gt/t1_mapped.csv\"\n",
    "t1_ont_graph = clean_ontology_edges(obonet.read_obo( \"/data/rashika/CAFA4/uniprot/goa_2024-02-09/go-basic.obo\")) # data-version: releases/2024-01-17\n",
    "\n",
    "BM_path = \"/home/rashika/CAFA4/eval/benchmarks/\"\n",
    "BM_GO_path = \"/home/rashika/CAFA4/eval/benchmarks_GO/\"\n",
    "\n",
    "# Create BM lists\n",
    "#t1_eval = create_bm_lists(shawn_t0_mapped_ann, t1_mapped_ann, t0_ont_graph, t1_ont_graph, roots, BM_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a62b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_code(directory):\n",
    "        image_code = ''\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for sub_dir in dirs:\n",
    "                print(sub_dir)\n",
    "                print(directory +sub_dir)\n",
    "                for _, _, files_sub_dir in os.walk(directory +sub_dir):\n",
    "                    for file in files_sub_dir:\n",
    "                        print(str(file))\n",
    "                        if file.endswith(\".png\"):\n",
    "                            image_path = os.path.join(root, file)\n",
    "                            folder = os.path.basename(root)\n",
    "                            image_code += \"\\\\begin{subfigure}[b]{0.3\\\\textwidth}\\n\"\n",
    "                            image_code += \"\\\\centering\\n\"\n",
    "                            image_code += \"\\\\includegraphics[width=\\\\textwidth]{\" + image_path + \"}\\n\"\n",
    "                            image_code += \"\\\\caption{Figure in \\\\texttt{\" + folder + \"}}\\n\"\n",
    "                            image_code += \"\\\\label{fig:\" + folder + \":\" + file + \"}\\n\"\n",
    "                            image_code += \"\\\\end{subfigure}\\n\"\n",
    "            return image_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03901170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to generate image code for current directory\n",
    "latex_image_code = generate_image_code('/home/rashika/CAFA4/eval/plots_ALL/f/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_image_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb7955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
