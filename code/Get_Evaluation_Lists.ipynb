{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a34777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annot(goa_file, n_skip = 9,out_file=\"extracted.csv\", col_list=[1, 4]):\n",
    "    \"\"\"\n",
    "    Extracts columns from a GOA file and writes them to a new file.\n",
    "\n",
    "    Parameters:\n",
    "    - goa_file: The input GOA file name.\n",
    "    - n_skip: No. of rows to be skipped. Default value 9.\n",
    "    - out_file: (Optional) The output file name. Defaults to 'extracted.csv'.\n",
    "    - col_list: List of column IDs to be extracted, e.g., [1, 5]. Defaults to [1, 4].\n",
    "    \"\"\"\n",
    "    with gzip.open(goa_file, 'rt') as f:\n",
    "        # Skip the first 8 lines\n",
    "        for _ in range(n_skip):\n",
    "            next(f)\n",
    "\n",
    "        # Create a CSV reader object with tab delimiter\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "\n",
    "        # Open the output file for writing\n",
    "        with open(out_file, 'w') as outfile:\n",
    "            # Create a CSV writer object\n",
    "            writer = csv.writer(outfile, delimiter='\\t')\n",
    "            \n",
    "            # Iterate over each row in the reader\n",
    "            for row in reader:\n",
    "                # Extract the specified columns\n",
    "                extracted_columns = [row[i] for i in col_list]\n",
    "\n",
    "                # Write the extracted columns to the output file\n",
    "                writer.writerow(extracted_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab76a9",
   "metadata": {},
   "source": [
    "### Extract Entry ID, GO annotation, and ontology type from the gaf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d894b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output file paths\n",
    "t0_out_dir = '/data/rashika/CAFA4/uniprot/goa_2020_Jan_03/'\n",
    "t0_input_file = t0_out_dir + 'goa_uniprot_all.gaf.gz'\n",
    "t0_output_file = t0_out_dir + 'extracted_columns.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_col_list = [1,4, 8]\n",
    "n_skip = 8\n",
    "\n",
    "#extract_annot(t0_input_file, n_skip, t0_output_file, t0_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f1809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output file paths\n",
    "t1_out_dir = '/data/rashika/CAFA4/uniprot/goa_2024-02-09/'\n",
    "t1_input_file = t1_out_dir + 'goa_uniprot_all.gaf.gz'\n",
    "t1_output_file = t1_out_dir + 'extracted_columns.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_col_list  = [1,3,4,6,8]\n",
    "n_skip = 9\n",
    "#extract_annot(t1_input_file, n_skip, t1_output_file, t1_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fdc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract annotations from the file used by Shawn\n",
    "shawn_t0_dir = '/data/yisupeng/sharing/cafa4/'\n",
    "in_file = shawn_t0_dir + 'goa_uniprot_all_02142020.gaf.gz'\n",
    "shawn_out_file = '/data/rashika/CAFA4/uniprot/'+ 'shawn_extracted_columns.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8963901",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list  = [1,4, 6, 8]\n",
    "n_skip = 8\n",
    "#extract_annot(in_file, n_skip, shawn_out_file, col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a1527",
   "metadata": {},
   "source": [
    "### Map the Extracted annotations to the CAFA targets (by Entry ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88deb8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_goa_to_cafa_ids(file_path, mapping_file, primary_id_column, out_path, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Read a CSV file in chunks, map the primary ID to a mapping file, and keep the rows that can be mapped.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the CSV file.\n",
    "    - mapping_file: Path to the mapping file (CSV).\n",
    "    - primary_id_column: Name of the column containing the primary ID in the mapping file.\n",
    "    - out_path: Path to the output file.\n",
    "    - chunk_size: Size of each chunk. Defaults to 100,000 lines.\n",
    "    \"\"\"\n",
    "    # Read the mapping file into a DataFrame\n",
    "    mapping_df = pd.read_csv(mapping_file, sep = \",\", header = 0)\n",
    "    mapping_df.columns = [\"Entry\", \"CAFA4_ID\"]\n",
    "\n",
    "    # Extract the primary IDs from the mapping file and convert to a set for efficient lookup\n",
    "    id_set = set(mapping_df[\"Entry\"])\n",
    "\n",
    "    # Initialize an empty list to store filtered chunk dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Read the CSV file in chunks\n",
    "    #flag = 0\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size, sep = \"\\t\"):\n",
    "        # Filter the chunk based on whether the primary ID can be found in the mapping file\n",
    "        filtered_chunk = chunk[chunk.iloc[:,primary_id_column].isin(id_set)]\n",
    "        filtered_chunk = filtered_chunk.drop_duplicates().copy()\n",
    "        dfs.append(filtered_chunk)\n",
    "        #print(chunk.iloc[:,primary_id_column])\n",
    "        #print(list(id_set)[:10])\n",
    "        #flag+=1\n",
    "        #if flag==100:\n",
    "        #    break\n",
    "\n",
    "    # Concatenate all the filtered chunk dataframes into a single dataframe\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Write the final dataframe to the output file\n",
    "    df.to_csv(out_path, index=False, sep = \"\\t\")\n",
    "\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60228f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mapping_file = \"/data/rashika/CAFA4/CAFA4_gt/Target_Entry_map.csv\"\n",
    "\n",
    "#Mapping_df = pd.read_csv(Mapping_file,  sep = ',', header = None)\n",
    "#Mapping_df.columns = [\"Entry\", \"CAFA4_ID\"]\n",
    "\n",
    "t1_mapped_ann = \"/data/rashika/CAFA4/CAFA4_gt/t1_ann.csv\"\n",
    "t0_mapped_ann = \"/data/rashika/CAFA4/CAFA4_gt/t0_ann.csv\"\n",
    "shawn_t0_mapped_ann = \"/data/rashika/CAFA4/CAFA4_gt/shawn_t0_ann.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db763c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clara_Entry_IDs = \"/data/rashika/CAFA4/CAFA4_gt/Entry.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clara_Entry_IDs = pd.read_csv(Clara_Entry_IDs,  sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0bfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map t1 annotations\n",
    "#map_goa_to_cafa_ids(t1_output_file, Mapping_file, 0, t1_mapped_ann )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf454e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map t0 annotations\n",
    "#map_goa_to_cafa_ids(t0_output_file, Mapping_file, 0, t0_mapped_ann )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef03582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Shawn's annotations\n",
    "#map_goa_to_cafa_ids(shawn_out_file, Mapping_file, 0, shawn_t0_mapped_ann )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57db1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://geneontology.org/docs/guide-go-evidence-codes/\n",
    "#Exp_codes = ['EXP', 'IDA', 'IMP', 'IGI', 'IEP', 'TAS', 'IC' ]\n",
    "Evidence_codes = ['EXP', 'IDA', 'IPI','IMP', 'IGI', 'IEP', 'TAS', 'IC', 'HTP', 'HDA', 'HMP', 'HGI', 'HEP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7673828",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(t1_mapped_ann,  sep = '\\t', header = None)\n",
    "t1.columns = ['Entry', 'edge', 'term', \"E_code\", \"aspect\"]\n",
    "\n",
    "\n",
    "# TO do\n",
    "\n",
    "# Write function to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a11ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EXP', 'HDA', 'HEP', 'HGI', 'HMP', 'HTP', 'IBA', 'IC', 'IDA',\n",
       "       'IEA', 'IEP', 'IGC', 'IGI', 'IKR', 'IMP', 'IPI', 'ISA', 'ISM',\n",
       "       'ISO', 'ISS', 'NAS', 'ND', 'RCA', 'TAS'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(t1.loc[:,\"E_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d6a0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NOT|acts_upstream_of', 'NOT|acts_upstream_of_or_within',\n",
       "       'NOT|acts_upstream_of_or_within_negative_effect',\n",
       "       'NOT|acts_upstream_of_or_within_positive_effect',\n",
       "       'NOT|colocalizes_with', 'NOT|contributes_to', 'NOT|enables',\n",
       "       'NOT|involved_in', 'NOT|is_active_in', 'NOT|located_in',\n",
       "       'NOT|part_of', 'acts_upstream_of',\n",
       "       'acts_upstream_of_negative_effect', 'acts_upstream_of_or_within',\n",
       "       'acts_upstream_of_or_within_negative_effect',\n",
       "       'acts_upstream_of_or_within_positive_effect',\n",
       "       'acts_upstream_of_positive_effect', 'colocalizes_with',\n",
       "       'contributes_to', 'enables', 'involved_in', 'is_active_in',\n",
       "       'located_in', 'part_of'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(t1.edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b7b55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024765697327871557"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t1.edge.apply(lambda x:\"NOT\" in x))/len(t1.edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc04de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = t1[t1.loc[:,\"E_code\"].isin(Evidence_codes)].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e98e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "shawn_t0 = pd.read_csv(shawn_t0_mapped_ann,  sep = '\\t', header = None)\n",
    "shawn_t0.columns = ['Entry', 'term', 'E_code','aspect']\n",
    "shawn_t0 = shawn_t0[shawn_t0.loc[:,\"E_code\"].isin(Evidence_codes)].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(shawn_t0['Entry']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59937da",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(t1['Entry']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce24373",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_mapped = pd.merge(t1, Mapping_df, on='Entry', how='inner')\n",
    "t1_mapped\n",
    "t1_mapped = t1_mapped.loc[:, [\"CAFA4_ID\", \"term\", \"aspect\", \"edge\"]]\n",
    "t1_mapped.to_csv('/data/rashika/CAFA4/CAFA4_gt/t1_mapped.csv', sep = \"\\t\", index=False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66638325",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_mapped = pd.merge(shawn_t0, Mapping_df, on='Entry', how='inner')\n",
    "t0_mapped\n",
    "t0_mapped = t0_mapped.loc[:, [\"CAFA4_ID\", \"term\", \"aspect\"]]\n",
    "t0_mapped.to_csv('/data/rashika/CAFA4/CAFA4_gt/t0_mapped.csv', sep = \"\\t\",index=False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4592920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
