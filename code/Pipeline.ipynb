{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "4a34777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import csv\n",
    "import io\n",
    "import argparse\n",
    "\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d66b5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Read a CSV file in chunks, map the primary ID to a mapping file, and keep the rows that can be mapped.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the CSV file.\n",
    "    - mapping_file: Path to the mapping file (CSV).\n",
    "    - primary_id_column: Index of the column containing the primary ID in the mapping file.\n",
    "    - out_path: Path to the output file.\n",
    "    - chunk_size: Size of each chunk. Defaults to 100,000 lines.\n",
    "    \"\"\"\n",
    "def map_goa_to_cafa_ids(file_path, mapping_file, primary_id_column, out_path, chunk_size=100000):\n",
    "    # Read the mapping file into a DataFrame\n",
    "    mapping_df = pd.read_csv(mapping_file, sep = \",\", header = 0)\n",
    "    mapping_df.columns = [\"DB Object ID\", \"CAFA4_ID\"]\n",
    "\n",
    "    # Extract the primary IDs from the mapping file and convert to a set for efficient lookup\n",
    "    id_set = set(mapping_df[\"DB Object ID\"])\n",
    "\n",
    "    # Initialize an empty list to store filtered chunk dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Read the CSV file in chunks\n",
    "    #flag = 0\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size, sep = \"\\t\"):\n",
    "        # Filter the chunk based on whether the primary ID can be found in the mapping file\n",
    "        filtered_chunk = chunk[chunk.iloc[:,primary_id_column].isin(id_set)]\n",
    "        filtered_chunk = filtered_chunk.drop_duplicates().copy()\n",
    "        dfs.append(filtered_chunk)\n",
    "\n",
    "    # Concatenate all the filtered chunk dataframes into a single dataframe\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    df_mapped = pd.merge(df, mapping_df, on='DB Object ID', how='inner')\n",
    "    print(\"Rows in the mapped file : \", len(df_mapped))\n",
    "    \n",
    "    df_mapped = df_mapped.loc[:, [\"CAFA4_ID\", \"GO ID\", \"Aspect\"]].copy()\n",
    "\n",
    "    # Write the final dataframe to the output file\n",
    "    df_mapped.to_csv(out_path, index=False, sep = \"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "7c6ef61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in the mapped file :  449581\n"
     ]
    }
   ],
   "source": [
    "def get_preprocess_cmd(gaf_path, out_path):\n",
    "    cmd = [\n",
    "    \"python3\",                 # Command to execute Python 3\n",
    "    \"preprocess_gaf.py\",       # Script to run\n",
    "    t0_gaf_file,  # Path to input file\n",
    "    \"--highTP\",\n",
    "    \"--out_path\", out_path,        # Output path parameter\n",
    "    #\"--evidence_codes\", \"EXP\", \"IDA\",   # Evidence codes parameter\n",
    "    #\"--extract_col_list\", \"DB Object ID\", \"Qualifier\"  # Extract column list parameter\n",
    "]\n",
    "    return cmd\n",
    "\n",
    "def run_process(command):\n",
    "    subprocess.run(command)\n",
    "    \n",
    "def make_bl_lists():\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define commands and log file names\n",
    "    work_dir = \"/data/rashika/CAFA4/\"\n",
    "    \n",
    "    #t0_gaf_file = work_dir + \"uniprot/raw_goa/sample_t0.gz\"\n",
    "    t0_gaf_file = work_dir + \"uniprot/raw_goa/t0/goa_uniprot_all.gaf.195.gz\"\n",
    "    t0_processed = work_dir + \"extracted_goa/t0_preprocessed.csv\"\n",
    "    log_t0 =  work_dir + \"log/log_preprocess_t0.txt\"\n",
    "    \n",
    "    t1_gaf_file = work_dir + \"uniprot/raw_goa/t1/goa_uniprot_all.gaf.gz\"\n",
    "    t1_processed = work_dir + \"extracted_goa/t1_preprocessed.csv\"\n",
    "    log_t1 = work_dir + \"log/log_preprocess_t1.txt\"\n",
    "    \n",
    "    \n",
    "    cmd_preprocess_t0 = get_preprocess_cmd(t0_gaf_file, t0_processed)\n",
    "    cmd_preprocess_t1 = get_preprocess_cmd(t1_gaf_file, t1_processed)\n",
    "    \n",
    "    # Preprocess both files, UNCOMMENT when needed\n",
    "    #run_process(cmd_preprocess_t0)\n",
    "    #run_process(cmd_preprocess_t1)\n",
    "    \n",
    "    \n",
    "    # Map the IDs of the processed \n",
    "    mapping_file = \"/data/rashika/CAFA4/CAFA4_gt/Target_Entry_map.csv\"\n",
    "    t0_mapped = work_dir + \"mapped/t0_mapped.csv\"\n",
    "    t1_mapped = work_dir + \"mapped/t1_mapped.csv\"\n",
    "    map_goa_to_cafa_ids(t0_processed, mapping_file, 0, t0_mapped, chunk_size=100000)\n",
    "    #map_goa_to_cafa_ids(t1_processed, mapping_file, 0, t1_mapped, chunk_size=100000)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
